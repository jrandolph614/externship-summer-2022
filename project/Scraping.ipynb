{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "aK7b1GntDmfp"
   },
   "outputs": [],
   "source": [
    "#this code only works for hashtags; most keywords that you would search have too many results for this code to run\n",
    "\n",
    "from congfig import * \n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "8I9AbgzFDmfs"
   },
   "outputs": [],
   "source": [
    "#swap these for your information, found on twitter api\n",
    "\n",
    "#API\n",
    "consumer_key = API_KEY\n",
    "#API Secret\n",
    "consumer_secret = API_SECRET\n",
    "#Token\n",
    "access_key = ACCESS_TOKEN\n",
    "#Token Secret\n",
    "access_secret= ACCESS_SECRET\n",
    "#Bearer\n",
    "bearer_token = BEARER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "s2HTRBN5Dmft"
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)\n",
    "#swap for your bearer token\n",
    "client = tweepy.Client(bearer_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "id": "NL7x_LHQDmft",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a862eebd-9a91-4581-f044-9eacf51a78a1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @CraigEpstein18: As he prepares for his sophomore season with #RFootball, @HamiltonKyonte talks spring ball and the transition from #RU‚Ä¶2022-03-28 15:35:08+00:00\n",
      "RT @CraigEpstein18: As he prepares for his sophomore season with #RFootball, @HamiltonKyonte talks spring ball and the transition from #RU‚Ä¶2022-03-28 15:34:26+00:00\n",
      "As he prepares for his sophomore season with #RFootball, @HamiltonKyonte talks spring ball and the transition from #RU wrestling. #Rutgers @RutgersRivals https://t.co/NPIsHAQo5v2022-03-28 15:28:19+00:00\n",
      "RT @RutgersRivals: .@HamiltonKyonte talks #Rutgers Football transition from #RU Wrestling and more as he prepares for year two on the banks‚Ä¶2022-03-28 15:22:22+00:00\n",
      ".@HamiltonKyonte talks #Rutgers Football transition from #RU Wrestling and more as he prepares for year two on the banks. \n",
      "\n",
      "https://t.co/VCY0Th2L5B2022-03-28 12:42:55+00:00\n",
      "RT @NameOfMy1stBand: Happening today! At 3:30 PM üé∏ Tix 10$ at the Rutgers Student Center 126 College Ave @RutgersU @Screamales @CityofNewBr‚Ä¶2022-03-26 20:47:35+00:00\n",
      "RT @NameOfMy1stBand: Happening today! At 3:30 PM üé∏ Tix 10$ at the Rutgers Student Center 126 College Ave @RutgersU @Screamales @CityofNewBr‚Ä¶2022-03-26 20:47:19+00:00\n",
      "Happening today! At 3:30 PM üé∏ Tix 10$ at the Rutgers Student Center 126 College Ave @RutgersU @Screamales @CityofNewBruns @thecorefm #Rutgers #thecore #thecorefm #corefest #RU #punk #nj\n",
      "\n",
      "Via @NB_Today \n",
      "\n",
      "https://t.co/IwaXBAENNh2022-03-26 18:05:37+00:00\n",
      "@MylestheMonster #ru #Rutgers  #rutgersbasketball. @__RHJR @caleb_mcco @paulmulcahy_4 @Geo_Baker_1 https://t.co/4GbLP5KGsW2022-03-26 02:20:53+00:00\n",
      "RT @IbkBender: There‚Äôs no words in the dictionary that can describe how happy I am for Naz. Once a player then a Coach. Our DB core wouldn‚Äô‚Ä¶2022-03-25 19:23:36+00:00\n",
      "There‚Äôs no words in the dictionary that can describe how happy I am for Naz. Once a player then a Coach. Our DB core wouldn‚Äôt be as successful without @Nas_Jones27 The glue to DBU! Congratulations DBU FOR LIFE #irvingtontuff #Rutgers #RU https://t.co/ebOtGBxga42022-03-25 19:14:10+00:00\n",
      "RT @RutgersRivals: üé¶NEW VIDEOüé¶\n",
      "\n",
      "#Rutgers DC @coachharasymiak met with the media today to discuss his journey to #RU, return to NJ and more.‚Ä¶2022-03-24 17:41:13+00:00\n",
      "üé¶NEW VIDEOüé¶\n",
      "\n",
      "#Rutgers DC @coachharasymiak met with the media today to discuss his journey to #RU, return to NJ and more. \n",
      "\n",
      "https://t.co/o75Jfr2yT12022-03-24 17:40:43+00:00\n",
      "Wearing my Rutgers super fan costume at #MarchMadness was the best decision ever. Met the #TexasTech hero that created their mascot in the 70's and their mascot in the 80's. Great people. Great times. #Rutgers #RU https://t.co/NRzY5yNof32022-03-21 05:54:43+00:00\n",
      "A real fan will still rock the gear the day after #RU #Rutgers #rutgersbasketball https://t.co/8jumDJBxLv2022-03-17 12:28:21+00:00\n",
      "How many OTs can a game have? I already lost an hour of sleep this week!!! #rutgers #RU\n",
      "\n",
      "** Also my cable crapped out at the end of reg and I've been watching the live tweets for info!!2022-03-17 03:57:44+00:00\n",
      "RT @C_MoSports: RON HARPER JUNYAAHHH #RU #MarchMadness #RUTGERS  https://t.co/vVCQn0IAsp2022-03-17 03:51:29+00:00\n",
      "How #RU #Rutgers2022-03-17 03:49:10+00:00\n",
      "RON HARPER JUNYAAHHH #RU #MarchMadness #RUTGERS  https://t.co/vVCQn0IAsp2022-03-17 03:47:51+00:00\n",
      "Can we please stop giving up layups #Rutgers #RU https://t.co/inNsvERuLy2022-03-17 03:08:56+00:00\n",
      "Let's Go #Rutgers! #RU #OnTheBanks \n",
      "#FirstFour2022-03-17 01:19:36+00:00\n",
      "@Muzixndmd üòÇüòÇ i am so excited, im freakin out üò¨üò¨üòäüòâüòÆüòúüòÇ\n",
      "\n",
      "#Rutgers #RU @RutgersMBB2022-03-17 00:29:11+00:00\n",
      "These refs for RU - Iowa have been fucking awful. Iowa was the better team but, what egos these refs have and I'm pretty sure they took Iowa -7. The calls were atrocious. Just absolutley awful officiating has to be the worst part of college basketball #Rutgers #Iowa #Big10 #RU2022-03-11 21:19:56+00:00\n",
      "RT @HueysKnightClub: TONIGHT! Let‚Äôs get loud for @RutgersMBB and keep voting for #barstoolbestbar! #BestBarHueys #BigTenChampionship #ru #r‚Ä¶2022-03-11 14:59:09+00:00\n",
      "RT @HueysKnightClub: TONIGHT! Let‚Äôs get loud for @RutgersMBB and keep voting for #barstoolbestbar! #BestBarHueys #BigTenChampionship #ru #r‚Ä¶2022-03-11 01:43:49+00:00\n",
      "RT @HueysKnightClub: TONIGHT! Let‚Äôs get loud for @RutgersMBB and keep voting for #barstoolbestbar! #BestBarHueys #BigTenChampionship #ru #r‚Ä¶2022-03-11 01:29:13+00:00\n",
      "https://t.co/R6s8sCC9xB \n",
      "\n",
      "#RU #rutgers #rutgersbasketball #bracketology #brackets #ncaab #bubbleteams2022-03-10 23:24:39+00:00\n",
      "RT @HueysKnightClub: TONIGHT! Let‚Äôs get loud for @RutgersMBB and keep voting for #barstoolbestbar! #BestBarHueys #BigTenChampionship #ru #r‚Ä¶2022-03-10 22:42:31+00:00\n",
      "RT @HueysKnightClub: TONIGHT! Let‚Äôs get loud for @RutgersMBB and keep voting for #barstoolbestbar! #BestBarHueys #BigTenChampionship #ru #r‚Ä¶2022-03-10 21:51:02+00:00\n",
      "RT @HueysKnightClub: TONIGHT! Let‚Äôs get loud for @RutgersMBB and keep voting for #barstoolbestbar! #BestBarHueys #BigTenChampionship #ru #r‚Ä¶2022-03-10 21:50:28+00:00\n",
      "RT @HueysKnightClub: TONIGHT! Let‚Äôs get loud for @RutgersMBB and keep voting for #barstoolbestbar! #BestBarHueys #BigTenChampionship #ru #r‚Ä¶2022-03-10 21:49:43+00:00\n",
      "RT @HueysKnightClub: TONIGHT! Let‚Äôs get loud for @RutgersMBB and keep voting for #barstoolbestbar! #BestBarHueys #BigTenChampionship #ru #r‚Ä¶2022-03-10 21:46:32+00:00\n",
      "RT @HueysKnightClub: TONIGHT! Let‚Äôs get loud for @RutgersMBB and keep voting for #barstoolbestbar! #BestBarHueys #BigTenChampionship #ru #r‚Ä¶2022-03-10 21:44:32+00:00\n",
      "TONIGHT! Let‚Äôs get loud for @RutgersMBB and keep voting for #barstoolbestbar! #BestBarHueys #BigTenChampionship #ru #rutgers #B1GMBBall #B1G #beatiowa https://t.co/3rCh66pz8L2022-03-10 21:40:36+00:00\n",
      "Come on @RutgersMBB don‚Äôt blow this game!! Let‚Äôs go #RU #Rutgers2022-03-06 18:53:58+00:00\n",
      "RAC is gonna be rockin‚Äô! \n",
      "\n",
      "#GardenStatement #Rutgers #RU #PoundNails #CHOP #BeatPennState #MarchMadness2022-03-06 16:27:25+00:00\n",
      "RT @RutgersRivals: Follow along with fellow #Rutgers Wrestling fans on our FREE #RU Wrestling message board linked below. \n",
      "\n",
      "üîó -- https://t.‚Ä¶2022-03-05 17:15:39+00:00\n",
      "Follow along with fellow #Rutgers Wrestling fans on our FREE #RU Wrestling message board linked below. \n",
      "\n",
      "üîó -- https://t.co/W1Mub9Y75s https://t.co/uVQW1jEW1N2022-03-05 16:27:03+00:00\n",
      "@RCourtClub @lukenathan33 #RutgersMBB used to be that guaranteed win so why not schedule it for Senior Day. \n",
      "\n",
      "- Not No More. \n",
      "\n",
      "Doesn‚Äôt get much better than going into someone else‚Äôs house and beating them on their own floor. \n",
      "\n",
      "Love Road Wins\n",
      "\n",
      "#Rutgers #RU #RUHoops2022-03-05 10:55:08+00:00\n",
      "My favorite part of #Rutgers upset wins is when fans dig up old tweets of people dogging #RU2022-03-03 15:59:32+00:00\n",
      ". @__RHJR  take that bow.... take your well deserved bow #RU #Rutgers https://t.co/arHwMcNPAc2022-03-03 02:16:24+00:00\n",
      "@RutgersMBB @__RHJR Let‚Äôs goooooooo!!!! @__RHJR is da man!!! Lets go #RU #Rutgers #ScarletKnights @RutgersMBB2022-03-03 02:11:36+00:00\n",
      "#Rutgershats #RUvsIND \n",
      "66-63!!!! Rutgers!!!!!!!! #Rutgers @RutgersMBB #RU https://t.co/x2kPURHxuW2022-03-03 02:11:29+00:00\n",
      "RT @NYPost_Serby: #RonHarperJr . on #MarchMadness push, dad's shadow, #Rutgers legacy #RU  https://t.co/DK1ZzdO2Ws2022-02-26 19:56:03+00:00\n",
      "#RonHarperJr . on #MarchMadness push, dad's shadow, #Rutgers legacy #RU  https://t.co/DK1ZzdO2Ws2022-02-26 19:55:19+00:00\n",
      "RT @CusserMother: Why the cuss isn't @RutgersMBB ranked after beating 4 ranked teams in a row?!? \n",
      "#RU #GardenStatement #Rutgers #Big102022-02-20 23:14:53+00:00\n",
      "Why the cuss isn't @RutgersMBB ranked after beating 4 ranked teams in a row?!? \n",
      "#RU #GardenStatement #Rutgers #Big102022-02-20 23:14:29+00:00\n",
      "#Rutgers Center Clifford Omoruyi posterizes #Purdue Center Zach Edey with the monster dunk #RU down 30-242022-02-20 23:01:18+00:00\n",
      "#Rutgers (16-9, 10-5) 4-wins vs. top-25 for in a row. 5-top-25 wins this season. #Illinois shot just 37% FG and 26% 3-pt. RU took advantage of IL missed shots, outrebounded IL 46-28 . Great all around performance by #RU starters. Need to see more from inconsistent Geo Baker #B1G2022-02-17 15:25:26+00:00\n",
      "Them boys are ballin‚Äô! Ranked reapers!  #Rutgers #RU #NJ2022-02-17 02:06:54+00:00\n",
      "Will keep saying it. Go look at NCAA tournament of #Rutgers compared to Iowa, Michigan, Michigan State and even Ohio State. It's not even close. So many more quality wins for #RU. Oh, Rutgers also beat all those teams.\n",
      "\n",
      "#ScarletKnights\n",
      "#RAC2022-02-17 01:41:47+00:00\n",
      "Im probably about a mile away and I could hear the screaming from the  RAC after that shot #Rutgers #RU2022-02-17 01:36:15+00:00\n",
      "#PoundNails\n",
      "\n",
      "#RU #RHoops #Rutgers #CBB #MBB #CHOP https://t.co/did51zk2Ix2022-02-17 00:49:58+00:00\n",
      "RT @RutgersRivals: #Rutgers Hoops does it again to win defeat No. 14 #Wisconsin, this was #RU's third ranked win in a row.\n",
      "\n",
      "https://t.co/KK‚Ä¶2022-02-12 21:56:36+00:00\n",
      "RT @RutgersRivals: #Rutgers Hoops does it again to win defeat No. 14 #Wisconsin, this was #RU's third ranked win in a row.\n",
      "\n",
      "https://t.co/KK‚Ä¶2022-02-12 21:36:19+00:00\n",
      "#Rutgers Hoops does it again to win defeat No. 14 #Wisconsin, this was #RU's third ranked win in a row.\n",
      "\n",
      "https://t.co/KKgB4ste342022-02-12 21:19:56+00:00\n",
      "Rutgers history, beating 3 Top 25 ranked teams consecutively\n",
      "\n",
      "#Rutgers #RU2022-02-12 21:11:53+00:00\n",
      "Let‚Äôs go #RU #Rutgers #ScarletKnights @RutgersMBB https://t.co/sadAwIlQIm2022-02-10 02:37:31+00:00\n",
      "RT @McQueenSports: #Rutgers defeats #OhioState 66-64. Another huge home win vs. a ranked #BigTen opponent. First #Purdue, then #MSU and now‚Ä¶2022-02-10 02:18:48+00:00\n",
      "#Rutgers defeats #OhioState 66-64. Another huge home win vs. a ranked #BigTen opponent. First #Purdue, then #MSU and now #OSU. Geo Baker led the way with 25-pts for #RU #NCAAB #CBB #B1G https://t.co/ve3EfNghDv2022-02-10 02:07:39+00:00\n",
      "Great article on Rutgers Basketball's @caleb_mcco\n",
      "#Rutgers #RU\n",
      "https://t.co/KJiMYzEudA2022-02-08 18:04:14+00:00\n",
      "Father Mulcahy is on fire today! Well done, Padre! üèÄ #RU @RutgersMBB #Rutgers @paulmulcahy_3 https://t.co/ViqYfG5bGr2022-02-05 23:05:03+00:00\n",
      "#RUvsNEB #Rutgers @RURec @RutgersMBB #RU \n",
      "Rutgers Wins! https://t.co/uKy0k0vv2w2022-01-30 02:08:45+00:00\n",
      "I legit have no idea how we won that game #Rutgers #RU #RUMBB2022-01-30 01:57:36+00:00\n",
      "Nothing like appreciating the finer details of a dog up-close! ü§§ üå≠\n",
      "\n",
      "üì∏ @DestinationDogs \n",
      "\n",
      "#destinationdogs #rutgers #ru #goru #rutgersuniversity #hotdog #food #hotdogs #burger #foodporn #instafood #foodie #hotdoglovers #foodphotography #njisntboring #thingstodo #nj https://t.co/eSYzRXGXjb2022-01-20 22:01:17+00:00\n",
      "RT @RivalsRichie: BIG hit to #Rutgers WRs room, as O'Neal caught two touchdowns over the past two years for #RU.2022-01-20 21:10:19+00:00\n",
      "RT @RivalsRichie: BIG hit to #Rutgers WRs room, as O'Neal caught two touchdowns over the past two years for #RU.2022-01-20 20:39:53+00:00\n",
      "RT @RivalsRichie: BIG hit to #Rutgers WRs room, as O'Neal caught two touchdowns over the past two years for #RU.2022-01-20 19:52:18+00:00\n",
      "RT @RivalsRichie: BIG hit to #Rutgers WRs room, as O'Neal caught two touchdowns over the past two years for #RU.2022-01-20 19:37:50+00:00\n",
      "BIG hit to #Rutgers WRs room, as O'Neal caught two touchdowns over the past two years for #RU. https://t.co/gn1iW8nfJi2022-01-20 19:35:13+00:00\n",
      "My type of Pizza üçïüòã \n",
      "\n",
      "üì∏ @marycello707 at @Panicosrestaurant.com \n",
      "Order online at https://t.co/fVkISy6bQ8 or call 732-545-6161 for delivery, pick-up, or to dine-in!‚†Ä\n",
      "\n",
      "#NewBrunswickNJ #Rutgers #RU #NJ #Foodie #pizza  food #pizzalover #pizzatime #instafood #italianfood #pasta https://t.co/Mc8i2secxx2022-01-18 19:55:02+00:00\n",
      "#Rutgers #basketball fans. After the program turned around and won an NCAA tournament game, why did Jacob Young and Myles Johnson transfer? They could have been quite good this season.\n",
      "\n",
      "#RU\n",
      "#RAC\n",
      "#ScarletKnights2022-01-14 18:03:02+00:00\n",
      "#RUtgers #Rutgers @RFootball #RU \n",
      "#knightclub #middlesex #MCC \n",
      "@MiddlesexUni  \n",
      "\n",
      "REAL TREASURE HUNT.   \n",
      "\n",
      "#FREE #teams #win #money #crypto https://t.co/SZ1h8h4nnR2022-01-13 00:10:42+00:00\n",
      "#rutgers #scarletknights @RutgersU #RU \n",
      "#eastonAve @middlesex_cc #MCC \n",
      "#JerseyCity #jersey4sale \n",
      "\n",
      "FIND THIS APES TREASURE. \n",
      "\n",
      "For real, gather your friends and open this MF VAULT!!!!!! https://t.co/UfIEoQVgE12022-01-13 00:08:58+00:00\n",
      "RT @RUSCIAA: #HappyNewYear from ‚ÄúR‚Äù #alumni family to yours! üéá #Rutgers #RU #SCIAA2022-01-01 21:59:31+00:00\n",
      "RT @RUSCIAA: #HappyNewYear from ‚ÄúR‚Äù #alumni family to yours! üéá #Rutgers #RU #SCIAA2022-01-01 06:48:56+00:00\n",
      "#HappyNewYear from ‚ÄúR‚Äù #alumni family to yours! üéá #Rutgers #RU #SCIAA2022-01-01 06:40:27+00:00\n",
      "WHAT EVEN IS A SAFETY IF THAT WASN'T IT?? #GatorBowl #Rutgers #WakeForest #RU #RUFOOTBALL2021-12-31 17:23:15+00:00\n",
      "10 minutes until kickoff and ESPN isn't even discussing the game.\n",
      "#Rutgers #RU #TaxSlayerGatorBowl #GatorBowl2021-12-31 15:57:53+00:00\n",
      "RT @NBCityCenter: Game Weekü™ì  \n",
      "\n",
      " @RFootball takes on Wake Forest for this Friday's Bowl Game beginning at 11am‚Äº \n",
      "\n",
      "#GoRU #NewBrunswickNJ #ne‚Ä¶2021-12-28 22:05:25+00:00\n",
      "Game Weekü™ì  \n",
      "\n",
      " @RFootball takes on Wake Forest for this Friday's Bowl Game beginning at 11am‚Äº \n",
      "\n",
      "#GoRU #NewBrunswickNJ #newjersey #centraljersey #centralNJ #rutgers #RUNATION #newjersey #rutgersuniversity #ru #newbrunswick #msu #fdu #nj #mcc #hubcity #njcollege #rutgersfootball https://t.co/5WXu5brqbP2021-12-28 20:01:28+00:00\n",
      "üé∂ Santa Dog is coming to Town üé∂\n",
      "\n",
      "üì∏ @destinationdogs\n",
      "\n",
      "#destinationdogs #rutgers #ru #goru #rutgersuniversity #hotdog #food #hotdogs #burger #foodporn #instafood #foodie #hotdoglovers #njisntboring #thingstodo #nj #newjersey #newbrunswick #newbrunswicknj #njeats #njfoodie https://t.co/8TeVN0RTAn2021-12-23 18:05:06+00:00\n",
      "RT @CJ_SmithGraphix: @23J_Taylor Jonathan Taylor Rutgers [COMMITTED] edit.  #smithgraphix #rutgers #ru #reppingjerseyinnj #committed #rb ht‚Ä¶2021-12-23 04:44:00+00:00\n",
      "RT @CJ_SmithGraphix: @23J_Taylor Jonathan Taylor Rutgers [COMMITTED] edit.  #smithgraphix #rutgers #ru #reppingjerseyinnj #committed #rb ht‚Ä¶2021-12-23 04:42:10+00:00\n",
      "RT @RutgersRivals: BREAKING: #Rutgers Baseball lands JUCO LHP Grant Besser out of Florida Southwestern. He is signed and will join the 2022‚Ä¶2021-12-17 16:42:59+00:00\n",
      "RT @RutgersRivals: BREAKING: #Rutgers Baseball lands JUCO LHP Grant Besser out of Florida Southwestern. He is signed and will join the 2022‚Ä¶2021-12-17 02:12:50+00:00\n",
      "RT @RutgersRivals: BREAKING: #Rutgers Baseball lands JUCO LHP Grant Besser out of Florida Southwestern. He is signed and will join the 2022‚Ä¶2021-12-17 02:11:11+00:00\n",
      "RT @RutgersRivals: BREAKING: #Rutgers Baseball lands JUCO LHP Grant Besser out of Florida Southwestern. He is signed and will join the 2022‚Ä¶2021-12-16 22:25:15+00:00\n",
      "RT @RutgersRivals: BREAKING: #Rutgers Baseball lands JUCO LHP Grant Besser out of Florida Southwestern. He is signed and will join the 2022‚Ä¶2021-12-16 21:44:26+00:00\n",
      "RT @RutgersRivals: BREAKING: #Rutgers Baseball lands JUCO LHP Grant Besser out of Florida Southwestern. He is signed and will join the 2022‚Ä¶2021-12-16 21:00:45+00:00\n",
      "RT @RutgersRivals: BREAKING: #Rutgers Baseball lands JUCO LHP Grant Besser out of Florida Southwestern. He is signed and will join the 2022‚Ä¶2021-12-16 20:20:23+00:00\n",
      "RT @RutgersRivals: BREAKING: #Rutgers Baseball lands JUCO LHP Grant Besser out of Florida Southwestern. He is signed and will join the 2022‚Ä¶2021-12-16 19:34:35+00:00\n",
      "RT @RutgersRivals: BREAKING: #Rutgers Baseball lands JUCO LHP Grant Besser out of Florida Southwestern. He is signed and will join the 2022‚Ä¶2021-12-16 18:50:33+00:00\n",
      "RT @RutgersRivals: BREAKING: #Rutgers Baseball lands JUCO LHP Grant Besser out of Florida Southwestern. He is signed and will join the 2022‚Ä¶2021-12-16 18:50:23+00:00\n",
      "BREAKING: #Rutgers Baseball lands JUCO LHP Grant Besser out of Florida Southwestern. He is signed and will join the 2022 #RU recruiting class.\n",
      "\n",
      "Stay tuned for more via @Hector_Baseball on https://t.co/wegN4qFyhs https://t.co/35g3VUARuv2021-12-16 18:49:18+00:00\n",
      "RT @RutgersRivals: The Jersey Boy stays home as #RU legacy TE Michael Higgins signs with #Rutgers Football\n",
      "\n",
      "@Mike_Higgins88 | @blairbucs |‚Ä¶2021-12-15 21:40:24+00:00\n",
      "RT @RutgersRivals: The Jersey Boy stays home as #RU legacy TE Michael Higgins signs with #Rutgers Football\n",
      "\n",
      "@Mike_Higgins88 | @blairbucs |‚Ä¶2021-12-15 17:27:55+00:00\n"
     ]
    },
    {
     "ename": "TooManyRequests",
     "evalue": "429 Too Many Requests\nToo Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2017-01-01T00:00:00Z\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-04-01T00:00:00Z\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweepy\u001b[38;5;241m.\u001b[39mPaginator(client\u001b[38;5;241m.\u001b[39msearch_all_tweets, query\u001b[38;5;241m=\u001b[39mquery,tweet_fields \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_annotations\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m'\u001b[39m], start_time\u001b[38;5;241m=\u001b[39mstart_time, end_time\u001b[38;5;241m=\u001b[39mend_time, max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(tweet\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(tweet\u001b[38;5;241m.\u001b[39mcreated_at))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py3.8\\lib\\site-packages\\tweepy\\pagination.py:48\u001b[0m, in \u001b[0;36mPaginator.flatten\u001b[1;34m(self, limit)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     47\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m PaginationIterator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m     49\u001b[0m                                    \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mdata:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py3.8\\lib\\site-packages\\tweepy\\pagination.py:98\u001b[0m, in \u001b[0;36mPaginationIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpagination_token\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pagination_token\n\u001b[1;32m---> 98\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevious_token \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_token\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_token \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_token\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py3.8\\lib\\site-packages\\tweepy\\client.py:1145\u001b[0m, in \u001b[0;36mClient.search_all_tweets\u001b[1;34m(self, query, **params)\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;124;03m\"\"\"search_all_tweets( \\\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m    query, *, end_time=None, expansions=None, max_results=None, \\\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m    media_fields=None, next_token=None, place_fields=None, \\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;124;03m.. _pagination: https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/paginate\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/2/tweets/search/all\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpansions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedia.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplace.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpoll.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msince_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msort_order\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtweet.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muntil_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTweet\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py3.8\\lib\\site-packages\\tweepy\\client.py:126\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[1;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, route, params\u001b[38;5;241m=\u001b[39m{}, endpoint_parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    123\u001b[0m                   json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    124\u001b[0m     request_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_params(params, endpoint_parameters)\n\u001b[1;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_auth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_type \u001b[38;5;129;01mis\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py3.8\\lib\\site-packages\\tweepy\\client.py:114\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[1;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(method, route, params, json, user_auth)\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests(response)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TwitterServerError(response)\n",
      "\u001b[1;31mTooManyRequests\u001b[0m: 429 Too Many Requests\nToo Many Requests"
     ]
    }
   ],
   "source": [
    "#test to see if you can scrape\n",
    "\n",
    "query = ['#Springfield #kilograms']\n",
    "start_time ='2017-01-01T00:00:00Z'\n",
    "end_time = '2022-04-01T00:00:00Z'\n",
    "for tweet in tweepy.Paginator(client.search_all_tweets, query=query,tweet_fields = ['context_annotations', 'created_at'], start_time=start_time, end_time=end_time, max_results=100).flatten(limit=1000):\n",
    "    print(str(tweet.text) + str(tweet.created_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = pd.read_csv('Hashtags.csv', header=0)\n",
    "location = list(hashtags[~hashtags['1'].isna()]['1'])\n",
    "content = list(hashtags[~hashtags['2'].isna()]['2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "B9rd3s3nDmfv"
   },
   "outputs": [],
   "source": [
    "#all combos of hashtags\n",
    "\n",
    "hitList = ['#'+str(x)+' '+'#'+str(y) for x in location for y in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "57fsp5K6Dmfx",
    "outputId": "f85ae2da-c679-4300-d40a-b62963529732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 254 20828\n"
     ]
    }
   ],
   "source": [
    "print(len(location), len(content), len(hitList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean out wildcard characters\n",
    "import re\n",
    "for s in hitList:\n",
    "    if re.findall('[(+*)]',s):\n",
    "        hitList.remove(s)\n",
    "for s in hitList:\n",
    "    if re.findall('[(+*)]',s):\n",
    "        print(s)     \n",
    "for s in hitList:\n",
    "    if re.findall('[(&)]',s):\n",
    "        hitList.remove(s)\n",
    "for s in hitList:\n",
    "    if re.findall('[(&)]',s):\n",
    "        print(s)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "K0ojXvDmDmfx",
    "outputId": "6de3e767-afd3-44b4-8dbd-0299385e4e83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#Syracuse #boylove', '#Syracuse #BLogo ', '#Syracuse #CLogo', '#Syracuse #GLogo', '#Syracuse #kiddie', '#Syracuse #little', '#Syracuse #teacup', '#Syracuse #teen', '#Syracuse #gayboy', '#Syracuse #tradeddropbox', '#Syracuse #young', '#Syracuse #trade', '#Syracuse #trading', '#Syracuse #girllove', '#Syracuse #Childlove', '#Syracuse #dropboxlinktrade', '#Syracuse #mega', '#Syracuse #megalink', '#Syracuse #lolita', '#Syracuse #twink', '#Syracuse #Prepago', '#Syracuse #student', '#Syracuse #sissy', '#Syracuse #petite', '#Syracuse #ladyboy', '#Syracuse #eros', '#Syracuse #Chad ', '#Syracuse #dating', '#Syracuse #meetup', '#Syracuse #mistress', '#Syracuse #dinnerdate', '#Syracuse #Escort', '#Syracuse #escort', '#Syracuse #escortads', '#Syracuse #luxurycompanion', '#Syracuse #skipthegames', '#Syracuse #virtualdates', '#Syracuse #flyme2you', '#Syracuse #flymetoyou', '#Syracuse #cityhotties']\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "#change the numbers in brackets to manually iterate through the hashtag list\n",
    "#searching everything at once results in too many requests\n",
    "\n",
    "#run all blocks here and down for each partial scrape. the only things that need to be changed each time \n",
    "#...scraping a chunk of hashtags are the numbers here and the filename to save\n",
    "\n",
    "hitList10 = hitList[0:40]\n",
    "print(hitList10)\n",
    "print(len(hitList10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "99YGi4x8Dmfy"
   },
   "outputs": [],
   "source": [
    "#need to run this each time you update hitlist10 so that it updates in the scraper\n",
    "def divide_chuncks(l,n):\n",
    "    for i in range(0,len(l),n):\n",
    "        yield l[i:i+n]\n",
    "matrix= list(divide_chuncks(hitList,40))\n",
    "ht1 = matrix\n",
    "date_since = '2017-01-01'\n",
    "numtweet = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "LlUYMgD-Dmfz"
   },
   "outputs": [],
   "source": [
    "#this is the scraper code, taken from the original manual scraper we used\n",
    "\n",
    "def scrape(ht1, date_since, numtweet):\n",
    " \n",
    "        # Creating DataFrame using pandas\n",
    "        #db1 is intermediary dataframe\n",
    "        db1 = pd.DataFrame(columns = ['text','date'])\n",
    " \n",
    "        # We are using .Cursor() to search\n",
    "        # through twitter for the required tweets.\n",
    "        # The number of tweets can be\n",
    "        # restricted using .items(number of tweets)\n",
    "        \n",
    "        tweets = tweepy.Paginator(client.search_all_tweets, query=ht1, tweet_fields = ['context_annotations', 'created_at'], start_time=start_time, max_results=100).flatten(limit=1000)\n",
    "            \n",
    " \n",
    "        # .Cursor() returns an iterable object. Each item in\n",
    "        # the iterator has various attributes\n",
    "        # that you can access to\n",
    "        # get information about each tweet\n",
    "        list_tweets = [tweet for tweet in tweets]\n",
    " \n",
    "        # Counter to maintain Tweet Count\n",
    "        i = 1\n",
    " \n",
    "        # we will iterate over each tweet in the\n",
    "        # list for extracting information about each tweet\n",
    "        for tweet in list_tweets:\n",
    "                text = tweet.text\n",
    "                date = tweet.created_at\n",
    "                #description = tweet.user.description\n",
    "                #followers = tweet.followers_count\n",
    "                #hashtags = tweet.entities['hashtags']\n",
    " \n",
    "                # Retweets can be distinguished by\n",
    "                # a retweeted_status attribute,\n",
    "                # in case it is an invalid reference,\n",
    "                # except block will be executed\n",
    "                \n",
    "                \n",
    "                #hashtext = list()\n",
    "                #for j in range(0, len(hashtags)):\n",
    "                 #      hashtext.append(hashtags[j]['text'])\n",
    "                \n",
    "                # Here we are appending all the\n",
    "                # extracted information in the DataFrame\n",
    "                ith_tweet = [text, date]\n",
    "                db.loc[len(db)] = ith_tweet\n",
    " \n",
    "                # Function call to print tweet data on screen\n",
    "                #printtweetdata(i, ith_tweet)\n",
    "                i = i+1\n",
    "        #db is master dataframe\n",
    "        db.append(db1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "J8O-dorPDmf0"
   },
   "outputs": [],
   "source": [
    "db = pd.DataFrame(columns=['text','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getRandom:\n",
    "    def __init__(self,l):\n",
    "        self.l = l\n",
    "        self.processed = []\n",
    "    def getRandomSample(self):\n",
    "        self.idx = np.random.randint(0,len(self.l))\n",
    "        self.sample = self.l.pop(self.idx)\n",
    "        self.processed.append(self.l.pop())\n",
    "        return self.sample\n",
    "    def returnProcessed(self):\n",
    "        return self.processed\n",
    "    def returnProcessedLength(self):\n",
    "        return len(self.processed)\n",
    "    def __iter__(self):\n",
    "        yield from self.l\n",
    "    def addToProcessed(self,ext):\n",
    "        self.processed.extend(ext)\n",
    "    def __len__(self):\n",
    "        return len(self.l)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove processed records so far\n",
    "mat = matrix[37:].copy()\n",
    "#initalize random class\n",
    "randomVal = getRandom(mat)\n",
    "# add in already processed records\n",
    "randomVal.addToProcessed(matrix[:37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "1cQ-Sz9LDmf0",
    "outputId": "050d16ce-8299-4ceb-dad9-75c956f8625b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix:\n",
      "['#Toga #SplendaDaddy', '#Toga #SplendaMama', '#Toga #SplendaMommy', '#Toga #LTA', '#Toga #NSA', '#Toga #ORO', '#Toga #PPM', '#Toga #hookup', '#Toga #POT', '#Toga #SA', '#Toga #STA', '#Toga #paypig', '#Toga #SB', '#Toga #RoadSugar', '#Toga #TravelingArrangement', '#Toga #SugarOnTheSide', '#Toga #bear', '#Toga #SD', '#Toga #SBSD', '#Toga #SDSB', '#Toga #PapiDulce', '#Toga #SaltDaddy', '#Toga #SaltMama', '#Toga #SaltMommy', '#Toga #MentoringArrangement', '#Toga #SugarBowl', '#Toga #SM', '#Toga #SR', '#Toga #Companion', '#Toga #companions', '#Toga #Companionship', '#Toga #SugarPup', '#Toga #BigDaddy', '#Toga #SugarBear', '#Toga #SugarDaddiesWanted', '#Toga #SugarMommyWanted', '#Toga #snap', '#Toga #snapchat', '#Binghamton #boylove', '#Binghamton #BLogo ']\n",
      "DB Len: 17169\n",
      "Matrix:\n",
      "['#Schenectady #lingerie', '#Schenectady #Freestyle', '#Schenectady #Daddy ', '#Schenectady #CollegeBaby', '#Schenectady #TuitionBaby', '#Schenectady #DS', '#Schenectady #AngelBaby', '#Schenectady #courtesan', '#Schenectady #findom', '#Schenectady #FWB', '#Schenectady #SplendaDaddy', '#Schenectady #SplendaMama', '#Schenectady #SplendaMommy', '#Schenectady #LTA', '#Schenectady #NSA', '#Schenectady #ORO', '#Schenectady #PPM', '#Schenectady #hookup', '#Schenectady #POT', '#Schenectady #SA', '#Schenectady #STA', '#Schenectady #paypig', '#Schenectady #SB', '#Schenectady #RoadSugar', '#Schenectady #TravelingArrangement', '#Schenectady #SugarOnTheSide', '#Schenectady #bear', '#Schenectady #SD', '#Schenectady #SBSD', '#Schenectady #SDSB', '#Schenectady #PapiDulce', '#Schenectady #SaltDaddy', '#Schenectady #SaltMama', '#Schenectady #SaltMommy', '#Schenectady #MentoringArrangement', '#Schenectady #SugarBowl', '#Schenectady #SM', '#Schenectady #SR', '#Schenectady #Companion', '#Schenectady #companions']\n",
      "DB Len: 17187\n",
      "Matrix:\n",
      "['#Utica #BBBJ', '#Utica #DP', '#Utica #CIM', '#Utica #BWC', '#Utica #BTW', '#Utica #BBBJTC', '#Utica #DATY', '#Utica #NIFOC ', '#Utica #GNOC ', '#Utica #freecams', '#Utica #ballbusting', '#Utica #nsfw', '#Utica #nude', '#Utica #nudes', '#Utica #chaturbate', '#Utica #lizard', '#Utica #Swoop ', '#Utica #roses', '#Utica #hooker', '#Utica #sexworker', '#Utica #sw', '#Utica #dickrate', '#Utica #pimp', '#Utica #ter', '#Utica #CU46 ', '#Utica #LMIRL ', '#Utica #ASL ', '#Utica #ass', '#Utica #cumtribute', '#Utica #fun', '#Utica #Thirsty ', '#Utica #DTF ', '#Utica #f4f', '#Utica #f4m', '#Utica #curves', '#Utica #cute', '#Utica #Club Penguin', '#Utica #53X ', '#Utica #m4f', '#Utica #m4m']\n",
      "DB Len: 17296\n",
      "Matrix:\n",
      "['#I87 #lbs', '#I87 #model', '#I87 #models', '#I87 #cashapp', '#I87 #venmo', '#I87 #WTTP ', '#I87 #s2r', '#I87 #live', '#I87 #streaming', '#I87 #webcam', '#I87 #webcamsex', '#I87 #cam2cam', '#I87 #camshow', '#I87 #livecam', '#I87 #camgirl', '#I87 #pr0n', '#I87 #porn', '#I87 #vids', '#I87 #buyingnudes', '#I87 #xxx', '#I87 #sellingcontent', '#I87 #sellingnudes', '#I87 #cam', '#I87 #boudoirphoto', '#I87 #buyingcontent', '#I87 #latex', '#I87 #pov', '#I87 #CBT', '#I87 #BJ', '#I87 #O', '#I87 #BBBJ', '#I87 #DP', '#I87 #CIM', '#I87 #BWC', '#I87 #BTW', '#I87 #BBBJTC', '#I87 #DATY', '#I87 #NIFOC ', '#I87 #GNOC ', '#I87 #freecams']\n",
      "DB Len: 17296\n",
      "Matrix:\n",
      "['#BinghamtonUniversity #friskyfriday', '#BinghamtonUniversity #sexysaturday', '#BinghamtonUniversity #thongthursday', '#BinghamtonUniversity #tittytuesday', '#BinghamtonUniversity #rt4rt', '#BinghamtonUniversity #mirrormonday', '#BinghamtonUniversity #sinday', '#BinghamtonUniversity #transpinay', '#BinghamtonUniversity #Smash ', '#BinghamtonUniversity #lover', '#BinghamtonUniversity #Bae ', '#BinghamtonUniversity #babes', '#BinghamtonUniversity #gfe', '#BinghamtonUniversity #secretgf', '#BinghamtonUniversity #escortagency', '#BinghamtonUniversity #escortdirectory', '#BinghamtonUniversity #escortgirls', '#BinghamtonUniversity #eroticmonkey', '#BinghamtonUniversity #eccie', '#BinghamtonUniversity #slixagirl', '#BinghamtonUniversity #ourhome2', '#BinghamtonUniversity #RentMen', '#BinghamtonUniversity #Ship ', '#BinghamtonUniversity #lost', '#BinghamtonUniversity #sad', '#BinghamtonUniversity #sadlife', '#BinghamtonUniversity #dark', '#BinghamtonUniversity #relaxation', '#BinghamtonUniversity #therapeutic', '#BinghamtonUniversity #massage', '#BinghamtonUniversity #massagefullservice', '#BinghamtonUniversity #eroticmassage', '#BinghamtonUniversity #massageplusplus', '#BinghamtonUniversity #sensualmassage', '#BinghamtonUniversity #kgs', '#BinghamtonUniversity #kilograms', '#BinghamtonUniversity #lbs', '#BinghamtonUniversity #model', '#BinghamtonUniversity #models', '#BinghamtonUniversity #cashapp']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [194]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m c \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m waiting:\n\u001b[1;32m---> 18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m() \u001b[38;5;241m-\u001b[39m start) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     19\u001b[0m             waiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     20\u001b[0m             start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#iterates through list for scraping\n",
    "#this counts to the length of ht1 while scraping then keyboardinterrupts\n",
    "#\"toomanyrequests\" prints between numbers & can print multiple times. this is correct\n",
    "#if \"toomanyrequests\" prints without numbers, wait 15 minutes, and try scraping again\n",
    "#if db is empty but numbers are printed until len(hitList10), there are no tweets in this set of pairs\n",
    "\n",
    "\n",
    "for i in range(len(randomVal)):\n",
    "    m = randomVal.getRandomSample()\n",
    "    c=0\n",
    "    start=-1\n",
    "    waiting =False\n",
    "    print('Matrix:')\n",
    "    print(m)\n",
    "    \n",
    "    while c <= len(m)-1:\n",
    "        if waiting:\n",
    "            if (time.time() - start) >= 1:\n",
    "                waiting = False\n",
    "                start = time.time()\n",
    "        else:\n",
    "            try: \n",
    "                scrape(m[c], date_since, numtweet)\n",
    "                c += 1\n",
    "            except tweepy.errors.TooManyRequests:\n",
    "                waiting = True\n",
    "                start = time.time()\n",
    "    print('DB Len:', len(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "MZ2YgQm0Dmf0",
    "outputId": "c8e5ca8b-36d5-4cac-e46e-370313eeb054"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To a GREAT Friend &amp;amp; Kind Heart, here are A...</td>\n",
       "      <td>2020-12-07 15:57:29+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remember when #Syracuse was a football blue bl...</td>\n",
       "      <td>2020-10-20 22:46:37+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Syracuse #orange #autograph #floyd #little #j...</td>\n",
       "      <td>2020-04-18 08:39:16+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @FloppZilla: Legends! #GOAT #SYRACUSE #Cson...</td>\n",
       "      <td>2019-04-14 14:28:29+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FloppZilla: Legends! #GOAT #SYRACUSE #Cson...</td>\n",
       "      <td>2019-04-14 05:15:17+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17291</th>\n",
       "      <td>RT @TramontaneCafe: Hey! #Baking friends:WHY  ...</td>\n",
       "      <td>2017-03-08 08:35:07+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>RT @TramontaneCafe: Hey! #Baking friends:WHY  ...</td>\n",
       "      <td>2017-03-07 21:16:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17293</th>\n",
       "      <td>Hey! #Baking friends:WHY  FORK LINES IN #PEANU...</td>\n",
       "      <td>2017-03-06 15:54:05+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17294</th>\n",
       "      <td>Rawr!!! Check out these adorable costumes! #Ha...</td>\n",
       "      <td>2020-10-31 23:10:41+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17295</th>\n",
       "      <td>#cosibellanail #cosibellanailspa #shelbytownsh...</td>\n",
       "      <td>2019-09-08 20:12:25+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17296 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      To a GREAT Friend &amp; Kind Heart, here are A...   \n",
       "1      remember when #Syracuse was a football blue bl...   \n",
       "2      #Syracuse #orange #autograph #floyd #little #j...   \n",
       "3      RT @FloppZilla: Legends! #GOAT #SYRACUSE #Cson...   \n",
       "4      RT @FloppZilla: Legends! #GOAT #SYRACUSE #Cson...   \n",
       "...                                                  ...   \n",
       "17291  RT @TramontaneCafe: Hey! #Baking friends:WHY  ...   \n",
       "17292  RT @TramontaneCafe: Hey! #Baking friends:WHY  ...   \n",
       "17293  Hey! #Baking friends:WHY  FORK LINES IN #PEANU...   \n",
       "17294  Rawr!!! Check out these adorable costumes! #Ha...   \n",
       "17295  #cosibellanail #cosibellanailspa #shelbytownsh...   \n",
       "\n",
       "                           date  \n",
       "0     2020-12-07 15:57:29+00:00  \n",
       "1     2020-10-20 22:46:37+00:00  \n",
       "2     2020-04-18 08:39:16+00:00  \n",
       "3     2019-04-14 14:28:29+00:00  \n",
       "4     2019-04-14 05:15:17+00:00  \n",
       "...                         ...  \n",
       "17291 2017-03-08 08:35:07+00:00  \n",
       "17292 2017-03-07 21:16:10+00:00  \n",
       "17293 2017-03-06 15:54:05+00:00  \n",
       "17294 2020-10-31 23:10:41+00:00  \n",
       "17295 2019-09-08 20:12:25+00:00  \n",
       "\n",
       "[17296 rows x 2 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs all scraped tweets\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "02ndGlPhDmf1"
   },
   "outputs": [],
   "source": [
    "# change csv filename every time you run this cell block\n",
    "filename = r'data/tenthrun.csv'\n",
    "db.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Scraping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
